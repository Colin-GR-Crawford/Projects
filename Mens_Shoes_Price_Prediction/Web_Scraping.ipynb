{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "In this notebook I will extract data regarding mens shoes from 5 different retailers sites and store the realted information in a local database. For further details surrounding the project please see the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import unicodedata\n",
    "from sqlalchemy import create_engine\n",
    "import datetime as dt\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create a connection to my local database.\n",
    "engine=create_engine('postgresql://localhost:5432/capstone_project')\n",
    "c=engine.connect()\n",
    "conn=c.connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This modifies the request that urllib sends to the websites when extracting information, as some websites will \n",
    "## block any programs that do not fit the profile of a normal web browser.\n",
    "from urllib import FancyURLopener\n",
    "class MyOpener(FancyURLopener):\n",
    "    version=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36\\\n",
    "                (KHTML, like Gecko) Chrome/54.0.2840.98 Safari/537.36\"\n",
    "myopener = MyOpener()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to extract the following information from each of the sites:\n",
    "- Shoe Brand\n",
    "- Shoe Description\n",
    "- Shoe Price\n",
    "- Main Image URL\n",
    "- Alternative Image URL\n",
    "\n",
    "I will attempt to scrape every single mens shoes of each of the 5 websites, intially I will create a function customised to each of the sites, and then I will run theses function for the desired page count, convert the data to dataframes and then store them on a local database that has been set up in Postico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##MATCHES FASION\n",
    "def Matches_Scrape(pages):\n",
    "    Brand_List=[]\n",
    "    Description_List=[]\n",
    "    Price_List=[]\n",
    "    Image_URL_List=[]\n",
    "    Image_URL_List_Alt=[]\n",
    "    for a in range(1,pages):\n",
    "        url=''.join(['http://www.matchesfashion.com/mens/shop/shoes?page=',str(a),'&noOfRecordsPerPage=240&sort='])\n",
    "        page = myopener.open(url)\n",
    "        soup = BeautifulSoup(page)\n",
    "        \n",
    "        ## Brand Data\n",
    "        Brand_Data=soup.find_all('div',{'class':'lister__item__title'})\n",
    "        for x in Brand_Data:\n",
    "            Brand_List.append(x.text)\n",
    "            \n",
    "        ## Description Data\n",
    "        Description_Data=soup.find_all('div',{'class':'lister__item__details'})\n",
    "        for x in Description_Data:\n",
    "            Description_List.append(x.text)\n",
    "        \n",
    "        ## Price Data\n",
    "        Price_Data=soup.find_all('div',{'class':'lister__item__price'})\n",
    "        for x in Price_Data:\n",
    "            s=x.text.strip('\\n')\n",
    "            Price_List.append(unicodedata.normalize('NFKD', s).encode('ascii','ignore'))   \n",
    "        \n",
    "        ## Image Data\n",
    "        ##URL number changes 1-2 for alt image\n",
    "        Image_Data=soup.find_all('img',{'class':'lazy'})\n",
    "        for x in Image_Data:\n",
    "            s=''.join(['http:',x['data-original']])\n",
    "            Image_URL_List.append(s)\n",
    "            s2=s.replace('_1_','_2_')\n",
    "            Image_URL_List_Alt.append(s2)\n",
    "            \n",
    "        time.sleep(5)\n",
    "            \n",
    "    return pd.DataFrame({'brand':Brand_List,\n",
    "                        'description':Description_List,\n",
    "                        'price':Price_List,\n",
    "                        'image_url':Image_URL_List,\n",
    "                        'image_url_alt':Image_URL_List_Alt,\n",
    "                        'retailer':'Matches_Fashion',\n",
    "                        'date':dt.datetime.today().strftime(\"%Y/%m/%d\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##NET-A-PORTER\n",
    "def NAP_Scrape(pages):\n",
    "    Brand_List=[]\n",
    "    Description_List=[]\n",
    "    Price_List=[]\n",
    "    Image_URL_List=[]\n",
    "    Image_URL_List_Alt=[]\n",
    "    for a in range(1,pages):\n",
    "        url=''.join(['https://www.mrporter.com/en-gb/mens/shoes?pn=',str(a)])\n",
    "        browser=webdriver.Firefox()\n",
    "        browser.get(url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        elem = browser.find_element_by_tag_name(\"body\")\n",
    "        no_of_pagedowns = 20\n",
    "\n",
    "        while no_of_pagedowns:\n",
    "            elem.send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(0.2)\n",
    "            no_of_pagedowns-=1\n",
    "\n",
    "        page_source=browser.page_source\n",
    "        soup = BeautifulSoup(page_source)\n",
    "        \n",
    "        ## Brand Data\n",
    "        Brand_Data=soup.find_all('span',{'class':'pl-products-item__text pl-products-item__text--brand pl-products-item__text--upper'})\n",
    "        for x in Brand_Data:\n",
    "            Brand_List.append(x.text)\n",
    "            \n",
    "        ## Description Data\n",
    "        Description_Data=soup.find_all('span',{'class':'pl-products-item__text pl-products-item__text--name'})\n",
    "        for x in Description_Data:\n",
    "            Description_List.append(x.text)        \n",
    "        \n",
    "        ## Price Data\n",
    "        Price_Data=soup.find_all('span',{'class':'pl-products-item__text pl-products-item__text--price'})\n",
    "        for x in Price_Data:\n",
    "            s=x.text.strip('\\n')\n",
    "            s=s.replace(',','')\n",
    "            Price_List.append(unicodedata.normalize('NFKD', s).encode('ascii','ignore'))   \n",
    "        \n",
    "        ## Image Data\n",
    "        ##URL number changes fr-e2 for alt image\n",
    "        Image_Data=soup.find_all('div',{'class':'pl-products-item__img pl-products-item__spacing'})\n",
    "        for x in Image_Data:\n",
    "            s=''.join(['http:',x.img['src']])\n",
    "            Image_URL_List.append(s)\n",
    "            s2=s.replace('_fr_','_e2_')\n",
    "            Image_URL_List_Alt.append(s2)\n",
    "            \n",
    "        browser.quit()\n",
    "        time.sleep(5)\n",
    "            \n",
    "    return pd.DataFrame({'brand':Brand_List,\n",
    "                        'description':Description_List,\n",
    "                        'price':Price_List,\n",
    "                        'image_url':Image_URL_List,\n",
    "                        'image_url_alt':Image_URL_List_Alt,\n",
    "                        'retailer':'Net_A_Porter',\n",
    "                        'date':dt.datetime.today().strftime(\"%Y/%m/%d\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##FARFETCH\n",
    "def Farfetch_Scrape(pages):\n",
    "    Brand_List=[]\n",
    "    Description_List=[]\n",
    "    Price_List=[]\n",
    "    Image_URL_List=[]\n",
    "    Image_URL_List_Alt=[]\n",
    "    \n",
    "    for a in range(1,pages):\n",
    "        url=''.join(['https://www.farfetch.com/uk/shopping/men/shoes-2/items.aspx?ffref=hd_snav&view=180&page=',str(a)])\n",
    "        page = myopener.open(url)\n",
    "        soup = BeautifulSoup(page)\n",
    "        \n",
    "        ##Brand Data\n",
    "        Brand_Data=soup.find_all('h5',{'class':'listing-item-content-brand'})\n",
    "        for x in Brand_Data:\n",
    "            Brand_List.append(x.text)\n",
    "            \n",
    "        ## Description Data\n",
    "        Description_Data=soup.find_all('p',{'class':'listing-item-content-description'})\n",
    "        for x in Description_Data:\n",
    "            Description_List.append(x.text)        \n",
    "        \n",
    "        ## Price Data\n",
    "        Price_Data=soup.find_all('span',{'class':'listing-item-content-price'})\n",
    "        for x in Price_Data:\n",
    "            s=x.text.strip('\\n')\n",
    "            s=s.replace(',','')\n",
    "            Price_List.append(unicodedata.normalize('NFKD', s).encode('ascii','ignore'))   \n",
    "        \n",
    "        ## Image Data\n",
    "        Image_Data=soup.find_all('img',{'itemprop':'image'})\n",
    "        for x in Image_Data:\n",
    "            Image_URL_List.append(x['data-img'])\n",
    "            Image_URL_List_Alt.append(x['data-img-alt'])\n",
    "    \n",
    "    time.sleep(5)\n",
    "                        \n",
    "    return pd.DataFrame({'brand':Brand_List,\n",
    "                        'description':Description_List,\n",
    "                        'price':Price_List,\n",
    "                        'image_url':Image_URL_List,\n",
    "                        'image_url_alt':Image_URL_List_Alt,\n",
    "                        'retailer':'Farfetch',\n",
    "                        'date':dt.datetime.today().strftime(\"%Y/%m/%d\")})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly Harrods has all of the shoes on the sight on one very long page, I therefore do not need to pass in a page number variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##HARRODS\n",
    "def Harrods_Scrape():\n",
    "    Brand_List=[]\n",
    "    Description_List=[]\n",
    "    Price_List=[]\n",
    "    Image_URL_List=[]\n",
    "    Image_URL_List_Alt=[]\n",
    "    \n",
    "    url='http://www.harrods.com/shoes/men-all-shoes?sort=0&viewall=yes'\n",
    "    page = myopener.open(url)\n",
    "    soup = BeautifulSoup(page)\n",
    "    \n",
    "    \n",
    "    ## First Row Data\n",
    "    First_Row_Data=soup.find_all('ul',{'class':'products_row clearfix top '})\n",
    "    for y in First_Row_Data:\n",
    "        \n",
    "        ## Element Data\n",
    "        Element_Data= y.find_all('li')\n",
    "        for x in Element_Data:\n",
    "            \n",
    "            ## Brand Data\n",
    "            Brand_List.append(x.h3.span.text)\n",
    "            \n",
    "            ## Description Data\n",
    "            Description_List.append(x.h3('span',{'class':'product-name'})[0].text)\n",
    "            \n",
    "            ## Price Data\n",
    "            s=x('span',{'class':'price_all plp_price'})[0].text\n",
    "            Price_List.append(unicodedata.normalize('NFKD', s).encode('ascii','ignore'))\n",
    "            \n",
    "            ## Image Data\n",
    "            Image_URL_List.append(x.find_all('img')[1]['src'])\n",
    "            Image_URL_List_Alt.append(x.find_all('img')[1]['data-hover'])\n",
    "    \n",
    "    ## Sections Data\n",
    "    Sections_Data=soup.find_all('ul',{'class':'products_row'})\n",
    "    \n",
    "    for y in Sections_Data:\n",
    "        ## Element Data\n",
    "        Element_Data= y.find_all('li')\n",
    "        for x in Element_Data:\n",
    "            \n",
    "            ## Brand Data\n",
    "            Brand_List.append(x.h3.span.text)\n",
    "            \n",
    "            ## Description Data\n",
    "            Description_List.append(x.h3('span',{'class':'product-name'})[0].text)\n",
    "            \n",
    "            ## Price Data\n",
    "            s=x('span',{'class':'price_all plp_price'})[0].text\n",
    "            Price_List.append(unicodedata.normalize('NFKD', s).encode('ascii','ignore'))\n",
    "            \n",
    "            ## Image Data\n",
    "            Image_URL_List.append(x.find_all('img')[1]['src'])\n",
    "            Image_URL_List_Alt.append(x.find_all('img')[1]['data-hover'])\n",
    "    \n",
    "    time.sleep(5)\n",
    "            \n",
    "    return pd.DataFrame({'brand':Brand_List,\n",
    "                        'description':Description_List,\n",
    "                        'price':Price_List,\n",
    "                        'image_url':Image_URL_List,\n",
    "                        'image_url_alt':Image_URL_List_Alt,\n",
    "                        'retailer':'Harrods',\n",
    "                        'date':dt.datetime.today().strftime(\"%Y/%m/%d\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##SAKSFIFTHAVENUE - specifc URLs (0,180,360,540,720,900,1080,1260,1440,1620,1800)\n",
    "def Saks_Scrape(pages):\n",
    "    Brand_List=[]\n",
    "    Description_List=[]\n",
    "    Price_List=[]\n",
    "    Image_URL_List=[]\n",
    "    Image_URL_List_Alt=[]\n",
    "    Image_URL_List_Alt1=[]\n",
    "    Image_URL_List_Alt2=[]\n",
    "    \n",
    "    \n",
    "    for p in pages:\n",
    "        url=''.join(['http://www.saksfifthavenue.com/Men/Shoes/shop/_/N-52flst/Ne-6lvnb5?FOLDER%3C%3Efolder_id=2534374306418205&Nao=',str(p)])\n",
    "        page = myopener.open(url)\n",
    "        soup = BeautifulSoup(page)\n",
    "        \n",
    "        ## Brand Data\n",
    "        Brand_Data=soup.find_all('span',{'class':'product-designer-name'})\n",
    "        for x in Brand_Data:\n",
    "            Brand_List.append(x.text)\n",
    "            \n",
    "        ## Description Data\n",
    "        Description_Data=soup.find_all('p',{'class':'product-description'})\n",
    "        for x in Description_Data:\n",
    "            Description_List.append(x.text)        \n",
    "        \n",
    "        ## Price Data\n",
    "        Price_Data=soup.find_all('span',{'class':'product-price'})\n",
    "        for x in Price_Data:\n",
    "            s=x.text.strip('\\n')\n",
    "            s=unicodedata.normalize('NFKD', s).encode('ascii','ignore')\n",
    "            s= ''.join(c for c in s if c not in ['\\n','\\t',' ','G','B','P','W','a','s'])\n",
    "            Price_List.append(s)  \n",
    "        \n",
    "        ## Image Data\n",
    "        Image_Data=soup.find_all('div',{'class':'image-container-large'})\n",
    "        for x in Image_Data:\n",
    "            Temp= x.img['params'].split(',')\n",
    "            Image_URL_List.append(Temp[1])     \n",
    "            Image_URL_List_Alt.append(Temp[2])\n",
    "                \n",
    "        time.sleep(5)\n",
    "                        \n",
    "    return pd.DataFrame({'brand':Brand_List,\n",
    "                        'description':Description_List,\n",
    "                        'price':Price_List,\n",
    "                        'image_url':Image_URL_List,\n",
    "                        'image_url_alt':Image_URL_List_Alt,\n",
    "                        'retailer':'Saks_Fifth_Avenue',\n",
    "                        'date':dt.datetime.today().strftime(\"%Y/%m/%d\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annacrawford/anaconda/envs/Zaincorp/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /Users/annacrawford/anaconda/envs/Zaincorp/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "## Scrape the data from 5 pages on the Matches Fashion website.\n",
    "Matches_Df=Matches_Scrape(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Store the data scrapped from Matches website to the local database.\n",
    "Matches_Df.to_sql('mens_shoes',engine,if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n",
      "'NoneType' object has no attribute 'path'\n"
     ]
    }
   ],
   "source": [
    "## Scrape shoes from 17 pages on the Mr. Porter website.\n",
    "NAP_Df=NAP_Scrape(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Store the data scrapped from Mr. Porter website to the local database.\n",
    "NAP_Df.to_sql('mens_shoes',engine,if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Scrape shoes from 17 pages on the Farfetch website.\n",
    "Farfetch_Df=Farfetch_Scrape(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Store the data scrapped from Farfetch website to the local database.\n",
    "Farfetch_Df.to_sql('mens_shoes',engine,if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Scrape shoes from 17 pages on the Harrods website.\n",
    "Harrods_Df=Harrods_Scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Store the data scrapped from Harrods website to the local database.\n",
    "Harrods_Df.to_sql('mens_shoes',engine,if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## For the saks fifth avenue site, specific urls are required, I will do this in two sets incase there are any issuse\n",
    "## I wont lose all of the scrapped data.\n",
    "Pagination_List=range(0,1021,60)\n",
    "Pagination_List2=range(1080,1561,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Scrape the data from Saks website (complete x2 with the two lists)\n",
    "Saks_Df=Saks_Scrape(Pagination_List2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Store the data scrapped from Saks website to the local database.\n",
    "Saks_Df.to_sql('mens_shoes',engine,if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have also added the locations of the details on some other ecommerce websites below in the case I would like to expand my dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##ASOS\n",
    "url='http://www.asos.com/men/shoes-boots-trainers/cat/?cid=4209&pge=0&pgesize=204'\n",
    "Brand_Data=soup.find_all('div',{'class':'name-fade'})\n",
    "#Description_Data=soup.find_all('p',{'class':'product-description'})\n",
    "Price_Data=soup.find_all('div',{'class':'price-wrap price-current'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##BOOHOO\n",
    "url='http://www.boohoo.com/restofworld/shoes/icat/mens-footwear#esp_hitsperpage=80'\n",
    "Brand_Data=soup.find_all('h3',{'class':'prod-name'})\n",
    "#Description_Data=soup.find_all('p',{'class':'product-description'})\n",
    "Price_Data=soup.find_all('div',{'class':'price-wrap price-current'})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Zaincorp]",
   "language": "python",
   "name": "Python [Zaincorp]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
